{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data augmentations\n",
    "\n",
    "This notebook will be used to test different data augmentation strategies. In your config file, you can modify the augmentations by adding `data_augmentation_options`. For example, I would like to add a random RGB to gray transform with a probability of 0.2:\n",
    "```\n",
    "data_augmentation_options {\n",
    "    random_rgb_to_gray {\n",
    "    probability: 0.2\n",
    "    }\n",
    "```\n",
    "\n",
    "A complete list of augmentation is provided the [`preprocessor.proto` file of the Tf Object Detection API](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recenter_image(image):\n",
    "    # ssd preprocessing\n",
    "    image += [123.68, 116.779, 103.939]\n",
    "    return image\n",
    "\n",
    "def display_instances(image, bboxes, classes):\n",
    "    image = recenter_image(image)\n",
    "    w, h, _ = image.shape\n",
    "    # resize the bboxes\n",
    "    bboxes[:, [0, 2]] *= w\n",
    "    bboxes[:, [1, 3]] *= h\n",
    "    \n",
    "    f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image.astype(np.uint8))\n",
    "    for bb, cl in zip(bboxes, classes):\n",
    "        y1, x1, y2, x2 = bb\n",
    "        rec = Rectangle((x1, y1), x2-x1, y2-y1, facecolor='none', edgecolor='r', linewidth=2)\n",
    "        ax.add_patch(rec)\n",
    "    plt.show()\n",
    "\n",
    "def display_batch(batch):\n",
    "    # get images, bboxes and classes\n",
    "    batched_images = batch[0]['image'].numpy()\n",
    "    batched_bboxes = batch[1]['groundtruth_boxes'].numpy()\n",
    "    batched_classes = batch[1]['groundtruth_classes'].numpy()\n",
    "    num_bboxes = batch[1]['num_groundtruth_boxes'].numpy()\n",
    "    batch_size = batched_images.shape[0]\n",
    "    for idx in range(batch_size):\n",
    "        display_instances(batched_images[idx, ...], \n",
    "                          batched_bboxes[idx, :num_bboxes[idx], :],\n",
    "                          batched_classes[idx, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one input path must be specified in `input_reader_config`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9c16b023d07a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/workspace/pipeline_new.config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/workspace/utils.py\u001b[0m in \u001b[0;36mget_train_input\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;31m# get the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/virtual_envs/sdc-c1-gpu-augment/lib/python3.7/site-packages/object_detection/inputs.py\u001b[0m in \u001b[0;36mtrain_input\u001b[0;34m(train_config, train_input_config, model_config, model, params, input_context)\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m       \u001b[0minput_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m       reduce_to_frame_fn=reduce_to_frame_fn)\n\u001b[0m\u001b[1;32m    899\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/virtual_envs/sdc-c1-gpu-augment/lib/python3.7/site-packages/object_detection/builders/dataset_builder.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(input_reader_config, batch_size, transform_input_data_fn, input_context, reduce_to_frame_fn)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_reader_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_record_input_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m       raise ValueError('At least one input path must be specified in '\n\u001b[0m\u001b[1;32m    216\u001b[0m                        '`input_reader_config`.')\n\u001b[1;32m    217\u001b[0m     def dataset_map_fn(dataset, fn_to_map, batch_size=None,\n",
      "\u001b[0;31mValueError\u001b[0m: At least one input path must be specified in `input_reader_config`."
     ]
    }
   ],
   "source": [
    "train_dataset = get_train_input(\"/home/workspace/pipeline_new.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8457e418fc51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdisplay_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for batch in train_dataset.take(1):\n",
    "    display_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
